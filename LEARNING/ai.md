## 语料

| 名称      | 含义 |
| --------- | ---- |
| coze      |      |
| dify      |      |
| ollama    |      |
| RAG       |      |
| langChain |      |
|           |      |
| 系统1     |      |
| 影视      |      |
|           |      |

## 神经网络

得分函数

损失函数

激活函数（非线性变换）：ReLU Sigmoid...

卷积神经网络

## Transformer

#### 注意力机制（Attention)

* **原理** ：注意力机制通过计算输入序列中每个位置与其他位置之间的相关性，来确定每个位置的重要性权重。它可以自适应地关注输入序列中的不同部分，从而更好地捕捉文本中的长距离依赖关系和语义信息。
* **计算方式** ：在 Transformer 中，注意力机制的计算主要涉及到查询（Query）、键（Key）和值（Value）三个向量。具体来说，对于每个输入位置，模型会将其映射为一个 Query 向量，同时将其他位置映射为 Key 和 Value 向量。然后，通过计算 Query 与 Key 之间的相似度得分，并对得分进行归一化，得到每个位置的注意力权重。最后，将注意力权重与 Value 向量进行加权求和，得到该位置的注意力输出。这个过程可以用以下公式表示：
  **\(Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)**
  其中，**Q**、**K**、**V**分别表示查询、键和值矩阵，**\(d_k\)**是键向量的维度，**softmax**函数用于将得分归一化为概率分布。
* **作用**
  * **灵活捕捉语义信息** ：能够根据具体的任务和输入文本，自动地调整对不同位置信息的关注度。例如，在处理具有嵌套结构或指代关系的句子时，注意力机制可以准确地将注意力集中在相关的词语上，从而更好地理解句子的语义。
  * **高效处理长序列数据** ：传统的神经网络在处理长序列数据时，容易出现梯度消失或爆炸的问题，导致难以捕捉长距离的依赖关系。而注意力机制可以直接对整个输入序列进行建模，无需像循环神经网络（RNN）或卷积神经网络（CNN）那样逐步处理序列，因此能够更有效地处理长序列数据，并且可以并行计算，大大提高了训练和推理的效率。

#### multi-headed机制
